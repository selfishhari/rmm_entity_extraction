{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*  \n",
    "*Licensed under the MIT License.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Using Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Start\n",
    "\n",
    "The running time shown in this notebook is on a Standard_NC6 Azure Deep Learning Virtual Machine with 1 NVIDIA Tesla K80 GPU. \n",
    "> **Tip**: If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`** to run the notebook on a small subset of the data and a smaller number of epochs. \n",
    "\n",
    "The table below provides some reference running time on different machine configurations.  \n",
    "\n",
    "|QUICK_RUN|Machine Configurations|Running time|\n",
    "|:---------|:----------------------|:------------|\n",
    "|True|4 **CPU**s, 14GB memory| ~ 2 minutes|\n",
    "|False|4 **CPU**s, 14GB memory| ~1.5 hours|\n",
    "|True|1 NVIDIA Tesla K80 GPUs, 12GB GPU memory| ~ 1 minute|\n",
    "|False|1 NVIDIA Tesla K80 GPUs, 12GB GPU memory| ~ 7 minutes |\n",
    "\n",
    "If you run into CUDA out-of-memory error or the jupyter kernel dies constantly, try reducing the `BATCH_SIZE` and `MAX_SEQ_LENGTH`, but note that model performance will be compromised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates how to fine tune [pretrained Transformer model](https://github.com/huggingface/transformers) for named entity recognition (NER) task. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, and model evaluation. \n",
    "\n",
    "The pretrained transformer of [BERT (Bidirectional Transformers for Language Understanding)](https://arxiv.org/pdf/1810.04805.pdf) architecture is used in this notebook. [BERT](https://arxiv.org/pdf/1810.04805.pdf) is a powerful pre-trained lanaguage model that can be used for multiple NLP tasks, including text classification, question answering, named entity recognition, etc. It's able to achieve state of the art performance with only a few epochs of fine tuning on task specific datasets.\n",
    "\n",
    "The figure below illustrates how BERT can be fine tuned for NER tasks. The input data is a list of tokens representing a sentence. In the training data, each token has an entity label. After fine tuning, the model predicts an entity label for each token in a given testing sentence. \n",
    "\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/bert_architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import scrapbook as sb\n",
    "import torch\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "from utils_nlp.dataset import wikigold\n",
    "from utils_nlp.common.timer import Timer\n",
    "from seqeval.metrics import classification_report\n",
    "from utils_nlp.models.transformers.named_entity_recognition import TokenClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# fraction of the dataset used for testing\n",
    "TEST_DATA_FRACTION = 0.3\n",
    "\n",
    "# sub-sampling ratio for training\n",
    "TRAIN_SAMPLE_RATIO = 1\n",
    "\n",
    "# sub-sampling ratio for testing\n",
    "TEST_SAMPLE_RATIO = 1\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "\n",
    "# update variables for quick run option\n",
    "if QUICK_RUN:\n",
    "    TRAIN_SAMPLE_RATIO = 0.1\n",
    "    TEST_SAMPLE_RATIO = 0.1\n",
    "    NUM_TRAIN_EPOCHS = 1\n",
    "\n",
    "# the data path used to save the downloaded data file\n",
    "DATA_PATH = TemporaryDirectory().name\n",
    "\n",
    "# the cache data path during find tuning\n",
    "CACHE_DIR = TemporaryDirectory().name\n",
    "\n",
    "# set random seeds\n",
    "RANDOM_SEED = 100\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# model configurations\n",
    "MODEL_NAME = \"bert-base-cased\"\n",
    "DO_LOWER_CASE = False\n",
    "MAX_SEQ_LENGTH = 200\n",
    "TRAILING_PIECE_TAG = \"X\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 16\n",
    "else:\n",
    "    BATCH_SIZE = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Traning & Testing Dataset\n",
    "\n",
    "The dataset used in this notebook is the [wikigold dataset](https://www.aclweb.org/anthology/W09-3302). The wikigold dataset consists of 145 mannually labelled Wikipedia articles, including 1841 sentences and 40k tokens in total. The dataset can be directly downloaded from [here](https://github.com/juand-r/entity-recognition-datasets/tree/master/data/wikigold). \n",
    "\n",
    "A helper function `load_dataset` downloads the raw wikigold data, splits it into training and testing datasets (also sub-sampling if the sampling ratio is smaller than 1.0), and then process for the transformer model. Everything is done in one function call, and you can use the processed training and testing Pytorch datasets to fine tune the model and evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader, label_map, test_dataset = wikigold.load_dataset(\n",
    "    local_path=DATA_PATH,\n",
    "    test_fraction=TEST_DATA_FRACTION,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    train_sample_ratio=TRAIN_SAMPLE_RATIO,\n",
    "    test_sample_ratio=TEST_SAMPLE_RATIO,\n",
    "    model_name=MODEL_NAME,\n",
    "    to_lower=DO_LOWER_CASE,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    max_len=MAX_SEQ_LENGTH,\n",
    "    trailing_piece_tag=TRAILING_PIECE_TAG,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_gpus=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "There are two steps to train a NER model using pretrained transformer model: 1). instantiate a TokenClassifier class which is a wrapper of the transformer using BERT architecture, and 2), fit the model using the preprocessed training dataset. The member method `fit` of TokenClassifier class is used to fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a TokenClassifier class for NER using pretrained transformer model\n",
    "model = TokenClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_labels=len(label_map),\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "# Fine tune the model using the training dataset\n",
    "with Timer() as t:\n",
    "    model.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        num_epochs=NUM_TRAIN_EPOCHS,\n",
    "        num_gpus=None,\n",
    "        local_rank=-1,\n",
    "        weight_decay=0.0,\n",
    "        learning_rate=5e-5,\n",
    "        adam_epsilon=1e-8,\n",
    "        warmup_steps=0,\n",
    "        verbose=True,\n",
    "        seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "print(\"Training time : {:.3f} hrs\".format(t.interval / 3600))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Testing Dataset\n",
    "\n",
    "The `predict` method of the TokenClassifier returns a Numpy ndarray of raw predictions. The shape of the ndarray is \\[`number_of_examples`, `sequence_length`, `number_of_labels`\\]. Each value in the ndarray is not normalized. Post-process will be needed to get the probability for each class label. Function `get_predicted_token_labels` will process the raw prediction and output the predicted labels for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as t:\n",
    "    preds = model.predict(\n",
    "        eval_dataloader=test_dataloader,\n",
    "        num_gpus=None,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "print(\"Prediction time : {:.3f} hrs\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the true token labels of the testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = model.get_true_test_labels(label_map=label_map, dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predicted labels for each token by calling member method `get_predicted_token_labels`, and generate the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = model.get_predicted_token_labels(\n",
    "    predictions=preds,\n",
    "    label_map=label_map,\n",
    "    dataset=test_dataset\n",
    ")\n",
    "\n",
    "report = classification_report(true_labels, \n",
    "              predicted_labels, \n",
    "              digits=2\n",
    ")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_splits = report.split('\\n')[-2].split()\n",
    "\n",
    "sb.glue(\"precision\", float(report_splits[2]))\n",
    "sb.glue(\"recall\", float(report_splits[3]))\n",
    "sb.glue(\"f1\", float(report_splits[4]))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
